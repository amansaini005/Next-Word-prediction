{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yH-BGIshMbSk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import pickle \n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "46aq6xuRN3oX",
        "outputId": "3e19960e-e98e-4120-e818-4442b842f168"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c19ee5b7-0512-4ad2-a39d-6abbd6349429\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c19ee5b7-0512-4ad2-a39d-6abbd6349429\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dracula1.txt to dracula1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Preprocess**"
      ],
      "metadata": {
        "id": "MVUFLcV0Oi6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"dracula1.txt\", 'r', encoding ='utf8')\n",
        "\n",
        "# store file in list\n",
        "lines =[]\n",
        "for i in file:\n",
        "  lines.append(i)\n",
        "\n",
        "# convert list to string\n",
        "data = \" \"\n",
        "for i in lines:\n",
        "  data = ' '.join(lines)\n",
        "\n",
        "# replace unnecessary stuff with space\n",
        "data  = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('\"', '').replace('\"', '')\n",
        "\n",
        "# remove unnecessary sapce\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "3j6zn6lvOfzy",
        "outputId": "91b2b59b-42f8-4142-b0d9-91967b3b9d5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Excuse me, I said, but the answer is here. I laid my hand on the type-written matter. When our sane and learned lunatic made that very Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late. Buda-Pesth seems a wonderful place, from the glimpse which I got of it from the train and the little I could walk through the streets. I feared to go very far from the station, as we had arrived late and would start as near the co'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugn-C5gdOgDL",
        "outputId": "720d658f-0715-42fd-8a0e-56b344a63100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12068"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "wlaxVdZ5RjMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "#saving tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGawqYAdRbQ8",
        "outputId": "08c8e727-130d-4293-b5f5-e021cf7131c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[262, 38, 2, 35, 19, 1, 263, 26, 48, 2, 264, 10, 162, 15, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js3Y9zFlRmij",
        "outputId": "c0c803e7-34d3-4f9a-a815-603eef99345b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2316"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBR_ksjhSpuY",
        "outputId": "db1e650e-c8dc-4f74-8b53-9889a563b3b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "  words = sequence_data[i-3: i+1]\n",
        "  sequences.append(words)\n",
        "\n",
        "print(\"length of sequences are \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10] \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkcvpBnyS5_Z",
        "outputId": "d2c91cc0-0b36-4e03-985c-32eb2b4b44e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of sequences are  2313\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[262,  38,   2,  35],\n",
              "       [ 38,   2,  35,  19],\n",
              "       [  2,  35,  19,   1],\n",
              "       [ 35,  19,   1, 263],\n",
              "       [ 19,   1, 263,  26],\n",
              "       [  1, 263,  26,  48],\n",
              "       [263,  26,  48,   2],\n",
              "       [ 26,  48,   2, 264],\n",
              "       [ 48,   2, 264,  10],\n",
              "       [  2, 264,  10, 162]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "  X.append(i[0:3])\n",
        "  y.append(i[3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)  \n",
        "\n"
      ],
      "metadata": {
        "id": "eEIiANVPT5KX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data \", X[:10])\n",
        "print( \"Response \", y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoMEp5fNUrrW",
        "outputId": "135ae8b8-a689-47d3-bfe3-d198b56f7a88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data  [[262  38   2]\n",
            " [ 38   2  35]\n",
            " [  2  35  19]\n",
            " [ 35  19   1]\n",
            " [ 19   1 263]\n",
            " [  1 263  26]\n",
            " [263  26  48]\n",
            " [ 26  48   2]\n",
            " [ 48   2 264]\n",
            " [  2 264  10]]\n",
            "Response  [ 35  19   1 263  26  48   2 264  10 162]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes = vocab_size)\n",
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xdPhK7PHBbR",
        "outputId": "d52051ae-683f-4612-ec59-8157b03ba82e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length = 3))\n",
        "model.add(LSTM(500, return_sequences = True))\n",
        "model.add(LSTM(500))\n",
        "model.add(Dense(500, activation = \"relu\"))\n",
        "model.add(Dense(vocab_size, activation = \"softmax\"))"
      ],
      "metadata": {
        "id": "dDQvRJ2QHBhk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_aestnH1X9",
        "outputId": "fa0b2b2c-17f9-4fbc-e270-c49831c760b9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 3, 10)             7670      \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 3, 500)            1022000   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 500)               2002000   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 500)               250500    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 767)               384267    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,666,437\n",
            "Trainable params: 3,666,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\"words.h5\", monitor = 'loss', verbose =1, save_best_only = True)\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.001))\n",
        "\n",
        "model.fit(X, y, epochs = 70, batch_size = 64, callbacks =[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMbeSnZ1H1h6",
        "outputId": "29f06ff9-ab30-4035-b2a0-f44f4c1eecb1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "37/37 [==============================] - ETA: 0s - loss: 6.2578\n",
            "Epoch 1: loss improved from inf to 6.25778, saving model to words.h5\n",
            "37/37 [==============================] - 8s 12ms/step - loss: 6.2578\n",
            "Epoch 2/70\n",
            "36/37 [============================>.] - ETA: 0s - loss: 5.7957\n",
            "Epoch 2: loss improved from 6.25778 to 5.79588, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.7959\n",
            "Epoch 3/70\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 5.7228\n",
            "Epoch 3: loss improved from 5.79588 to 5.74576, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.7458\n",
            "Epoch 4/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 5.7074\n",
            "Epoch 4: loss improved from 5.74576 to 5.70076, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.7008\n",
            "Epoch 5/70\n",
            "36/37 [============================>.] - ETA: 0s - loss: 5.6420\n",
            "Epoch 5: loss improved from 5.70076 to 5.64297, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.6430\n",
            "Epoch 6/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 5.6190\n",
            "Epoch 6: loss improved from 5.64297 to 5.61449, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.6145\n",
            "Epoch 7/70\n",
            "37/37 [==============================] - ETA: 0s - loss: 5.5643\n",
            "Epoch 7: loss improved from 5.61449 to 5.56427, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.5643\n",
            "Epoch 8/70\n",
            "36/37 [============================>.] - ETA: 0s - loss: 5.5115\n",
            "Epoch 8: loss improved from 5.56427 to 5.51359, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.5136\n",
            "Epoch 9/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 5.4258\n",
            "Epoch 9: loss improved from 5.51359 to 5.45645, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.4565\n",
            "Epoch 10/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 5.3991\n",
            "Epoch 10: loss improved from 5.45645 to 5.40664, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.4066\n",
            "Epoch 11/70\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 5.3258\n",
            "Epoch 11: loss improved from 5.40664 to 5.35074, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.3507\n",
            "Epoch 12/70\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 5.2701\n",
            "Epoch 12: loss improved from 5.35074 to 5.29497, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.2950\n",
            "Epoch 13/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 5.2299\n",
            "Epoch 13: loss improved from 5.29497 to 5.25146, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.2515\n",
            "Epoch 14/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 5.1982\n",
            "Epoch 14: loss improved from 5.25146 to 5.19608, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.1961\n",
            "Epoch 15/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 5.1279\n",
            "Epoch 15: loss improved from 5.19608 to 5.12988, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.1299\n",
            "Epoch 16/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 5.0874\n",
            "Epoch 16: loss improved from 5.12988 to 5.09336, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.0934\n",
            "Epoch 17/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 5.0100\n",
            "Epoch 17: loss improved from 5.09336 to 5.02415, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.0241\n",
            "Epoch 18/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 4.9651\n",
            "Epoch 18: loss improved from 5.02415 to 4.96498, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.9650\n",
            "Epoch 19/70\n",
            "37/37 [==============================] - ETA: 0s - loss: 4.8704\n",
            "Epoch 19: loss improved from 4.96498 to 4.87038, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.8704\n",
            "Epoch 20/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 4.7944\n",
            "Epoch 20: loss improved from 4.87038 to 4.81267, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.8127\n",
            "Epoch 21/70\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 4.7015\n",
            "Epoch 21: loss improved from 4.81267 to 4.67405, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 4.6740\n",
            "Epoch 22/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 4.5458\n",
            "Epoch 22: loss improved from 4.67405 to 4.53760, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.5376\n",
            "Epoch 23/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 4.4082\n",
            "Epoch 23: loss improved from 4.53760 to 4.39731, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.3973\n",
            "Epoch 24/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 4.2526\n",
            "Epoch 24: loss improved from 4.39731 to 4.26281, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.2628\n",
            "Epoch 25/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 4.1360\n",
            "Epoch 25: loss improved from 4.26281 to 4.13261, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.1326\n",
            "Epoch 26/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 3.9464\n",
            "Epoch 26: loss improved from 4.13261 to 3.96242, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.9624\n",
            "Epoch 27/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 3.8149\n",
            "Epoch 27: loss improved from 3.96242 to 3.81700, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.8170\n",
            "Epoch 28/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 3.6594\n",
            "Epoch 28: loss improved from 3.81700 to 3.67025, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.6703\n",
            "Epoch 29/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 3.4645\n",
            "Epoch 29: loss improved from 3.67025 to 3.47985, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.4798\n",
            "Epoch 30/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 3.3138\n",
            "Epoch 30: loss improved from 3.47985 to 3.33464, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.3346\n",
            "Epoch 31/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 3.1511\n",
            "Epoch 31: loss improved from 3.33464 to 3.18233, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.1823\n",
            "Epoch 32/70\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 2.9834\n",
            "Epoch 32: loss improved from 3.18233 to 3.04424, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 3.0442\n",
            "Epoch 33/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 2.8891\n",
            "Epoch 33: loss improved from 3.04424 to 2.93925, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.9392\n",
            "Epoch 34/70\n",
            "36/37 [============================>.] - ETA: 0s - loss: 2.7694\n",
            "Epoch 34: loss improved from 2.93925 to 2.77106, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.7711\n",
            "Epoch 35/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 2.5796\n",
            "Epoch 35: loss improved from 2.77106 to 2.61114, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.6111\n",
            "Epoch 36/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 2.4449\n",
            "Epoch 36: loss improved from 2.61114 to 2.45450, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4545\n",
            "Epoch 37/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 2.3709\n",
            "Epoch 37: loss improved from 2.45450 to 2.38440, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3844\n",
            "Epoch 38/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 2.1906\n",
            "Epoch 38: loss improved from 2.38440 to 2.21244, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.2124\n",
            "Epoch 39/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 1.9978\n",
            "Epoch 39: loss improved from 2.21244 to 2.01591, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.0159\n",
            "Epoch 40/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 1.9431\n",
            "Epoch 40: loss improved from 2.01591 to 1.94449, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.9445\n",
            "Epoch 41/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 1.8262\n",
            "Epoch 41: loss improved from 1.94449 to 1.83876, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.8388\n",
            "Epoch 42/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 1.7637\n",
            "Epoch 42: loss improved from 1.83876 to 1.78116, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7812\n",
            "Epoch 43/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 1.6341\n",
            "Epoch 43: loss improved from 1.78116 to 1.64551, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.6455\n",
            "Epoch 44/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 1.5335\n",
            "Epoch 44: loss improved from 1.64551 to 1.55256, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5526\n",
            "Epoch 45/70\n",
            "37/37 [==============================] - ETA: 0s - loss: 1.4507\n",
            "Epoch 45: loss improved from 1.55256 to 1.45068, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.4507\n",
            "Epoch 46/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 1.3564\n",
            "Epoch 46: loss improved from 1.45068 to 1.37476, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3748\n",
            "Epoch 47/70\n",
            "37/37 [==============================] - ETA: 0s - loss: 1.2715\n",
            "Epoch 47: loss improved from 1.37476 to 1.27153, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2715\n",
            "Epoch 48/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 1.1316\n",
            "Epoch 48: loss improved from 1.27153 to 1.15009, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1501\n",
            "Epoch 49/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 1.0284\n",
            "Epoch 49: loss improved from 1.15009 to 1.03840, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0384\n",
            "Epoch 50/70\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 0.9808\n",
            "Epoch 50: loss improved from 1.03840 to 0.99549, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.9955\n",
            "Epoch 51/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 0.9397\n",
            "Epoch 51: loss improved from 0.99549 to 0.95901, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.9590\n",
            "Epoch 52/70\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 0.8606\n",
            "Epoch 52: loss improved from 0.95901 to 0.86971, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.8697\n",
            "Epoch 53/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 0.7846\n",
            "Epoch 53: loss improved from 0.86971 to 0.79189, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.7919\n",
            "Epoch 54/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 0.7305\n",
            "Epoch 54: loss improved from 0.79189 to 0.75566, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.7557\n",
            "Epoch 55/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 0.7412\n",
            "Epoch 55: loss improved from 0.75566 to 0.74057, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.7406\n",
            "Epoch 56/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 0.6610\n",
            "Epoch 56: loss improved from 0.74057 to 0.66987, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.6699\n",
            "Epoch 57/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 0.6520\n",
            "Epoch 57: loss improved from 0.66987 to 0.66913, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.6691\n",
            "Epoch 58/70\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 0.7016\n",
            "Epoch 58: loss did not improve from 0.66913\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 0.7100\n",
            "Epoch 59/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 0.6422\n",
            "Epoch 59: loss improved from 0.66913 to 0.65402, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.6540\n",
            "Epoch 60/70\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 0.5598\n",
            "Epoch 60: loss improved from 0.65402 to 0.56742, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.5674\n",
            "Epoch 61/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 0.5230\n",
            "Epoch 61: loss improved from 0.56742 to 0.52098, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.5210\n",
            "Epoch 62/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 0.4439\n",
            "Epoch 62: loss improved from 0.52098 to 0.43706, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.4371\n",
            "Epoch 63/70\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 0.3723\n",
            "Epoch 63: loss improved from 0.43706 to 0.37248, saving model to words.h5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.3725\n",
            "Epoch 64/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 0.3232\n",
            "Epoch 64: loss improved from 0.37248 to 0.32540, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.3254\n",
            "Epoch 65/70\n",
            "37/37 [==============================] - ETA: 0s - loss: 0.3275\n",
            "Epoch 65: loss did not improve from 0.32540\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 0.3275\n",
            "Epoch 66/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 0.3153\n",
            "Epoch 66: loss improved from 0.32540 to 0.32253, saving model to words.h5\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.3225\n",
            "Epoch 67/70\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 0.3396\n",
            "Epoch 67: loss did not improve from 0.32253\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 0.3424\n",
            "Epoch 68/70\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 0.3583\n",
            "Epoch 68: loss did not improve from 0.32253\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 0.3680\n",
            "Epoch 69/70\n",
            "37/37 [==============================] - ETA: 0s - loss: 0.3500\n",
            "Epoch 69: loss did not improve from 0.32253\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 0.3500\n",
            "Epoch 70/70\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 0.3284\n",
            "Epoch 70: loss did not improve from 0.32253\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 0.3310\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2a3a5f7210>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np \n",
        "import pickle\n",
        "\n",
        "# load model and tokenize\n",
        "model = load_model('words.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_next_words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \" \"\n",
        "\n",
        "  for key, value in tokenizer.word_index.items():\n",
        "    if value == preds:\n",
        "      predicted_word = key\n",
        "      break\n",
        "\n",
        "  print(predicted_word)\n",
        "  return predicted_word  "
      ],
      "metadata": {
        "id": "SZXBYfwrMhH8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "\n",
        "  if text == '0':\n",
        "    print(\"Execution completed ...\")\n",
        "    break\n",
        "\n",
        "  else:\n",
        "    try:\n",
        "      text = text.split(\" \")\n",
        "      text = text[-3:]\n",
        "      print(text) \n",
        "\n",
        "      Predict_next_words(model, tokenizer, text)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"Error_occured: \" , e)\n",
        "\n",
        "      continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQBn4MLyMmvb",
        "outputId": "f9fdfeec-c37d-4dcf-9544-6b666949b612"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: Van Helsing smiled in \n",
            "['smiled', 'in', '']\n",
            "Error_occured:  in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1801, in predict_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1790, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1783, in run_step  **\n",
            "        outputs = model.predict_step(data)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1751, in predict_step\n",
            "        return self(x, training=False)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "        raise e.with_traceback(filtered_tb) from None\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n",
            "        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n",
            "\n",
            "    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)\n",
            "\n",
            "Enter your line: he asked politely as he\n",
            "['politely', 'as', 'he']\n",
            "stood\n",
            "Enter your line: but all shall\n",
            "['but', 'all', 'shall']\n",
            "be\n",
            "Enter your line: my carriage shall\n",
            "['my', 'carriage', 'shall']\n",
            "come\n",
            "Enter your line: and determined\n",
            "['and', 'determined']\n",
            "of\n",
            "Enter your line: and determined to \n",
            "['determined', 'to', '']\n",
            "follow\n",
            "Enter your line: and memory which makes\n",
            "['memory', 'which', 'makes']\n",
            "mental\n",
            "Enter your line: he asked politely\n",
            "['he', 'asked', 'politely']\n",
            "as\n",
            "Enter your line: 0\n",
            "Execution completed ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMZ5D2CrMhWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}